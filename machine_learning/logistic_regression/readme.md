# 一元回归

鉴于天书讲的太抽象，这里我接地气一点，提出一个实际的问题，然后我们解决这个实际的问题，然后你就懂里面的知识点了。

## 问题

现在在一个x-y轴上，存在几个点，现在我想用一个直线去拟合这几个点，使得这条直线的误差最小。

举个🌰，现在我有五个点：

| x | y | 
|---|---|
| 1 | 3 |
| 2 | 5 |
| 3 | 7 |
| 4 | 9 |
| 5 | 11 |

## 最小二乘法

### 原理

最小二乘法的核心思想是通过最小化误差的平方和来确定回归直线的参数。在一元线性回归中，我们假设回归直线方程为 $y = kx + b$
，其中 $k$ 是斜率，$b$ 是截距。

对于给定的 $n$ 个数据点 $(x_i, y_i)$（$i = 1,2,\cdots,n$），使用回归直线预测的值 $\hat{y}_i=kx_i + b$ 与真实值 $y_i$
之间存在误差 $e_i=y_i - \hat{y}_i=y_i - (kx_i + b)$。最小二乘法的目标就是找到合适的 $k$ 和 $b$
，使得误差平方和 $S(k,b)=\sum_{i = 1}^{n}e_i^2=\sum_{i = 1}^{n}(y_i - (kx_i + b))^2$ 达到最小。

### 求解公式

为了找到使 $S(k,b)$ 最小的 $k$ 和 $b$，我们分别对 $k$ 和 $b$ 求偏导数，并令偏导数等于 0。

- 对 $b$ 求偏导：
  令 $\frac{\partial S}{\partial b}=-2\sum_{i = 1}^{n}(y_i - (kx_i + b)) = 0$
  ，经过化简可得 $nb + k\sum_{i = 1}^{n}x_i=\sum_{i = 1}^{n}y_i$。

- 对 $k$ 求偏导：
  令 $\frac{\partial S}{\partial k}=-2\sum_{i = 1}^{n}(y_i - (kx_i + b))x_i = 0$
  ，化简后得到 $b\sum_{i = 1}^{n}x_i + k\sum_{i = 1}^{n}x_i^2=\sum_{i = 1}^{n}x_iy_i$。

联立上述两个方程求解，最终可以得到 $k$ 和 $b$ 的计算公式：

- 斜率 $k$ 的计算公式：
  $$k=\frac{\sum_{i = 1}^{n}(x_i-\bar{x})(y_i - \bar{y})}{\sum_{i = 1}^{n}(x_i-\bar{x})^2}$$

- 截距 $b$ 的计算公式：
  $$b=\bar{y}-k\bar{x}$$

其中，$\bar{x}=\frac{1}{n}\sum_{i = 1}^{n}x_i$ 是 $x$ 的均值，$\bar{y}=\frac{1}{n}\sum_{i = 1}^{n}y_i$ 是 $y$ 的均值。

### 优缺点

- **优点**：计算过程相对简单，并且存在解析解，能够直接通过公式计算出最优的 $k$ 和 $b$，无需进行复杂的迭代计算。
- **缺点**
  ：对异常值较为敏感，因为异常值的误差平方会在误差平方和的计算中占据较大比重，从而可能对回归直线的拟合效果产生较大影响。此外，当数据量非常大时，最小二乘法需要计算和存储大量的中间结果，计算复杂度较高，可能会面临内存和计算资源的瓶颈。

## 梯度下降法

### 原理

梯度下降法是一种迭代优化算法，其目的是寻找函数的最小值。在一元线性回归问题中，我们将误差平方和 $S(k,b)$ 视为关于 $k$ 和 $b$
的函数。梯度下降法的基本思路是从一个初始点开始，沿着函数的负梯度方向逐步更新 $k$ 和 $b$ 的值，直到达到局部最小值或者满足预设的停止条件。

函数在某一点的梯度是一个向量，其方向表示函数在该点上升最快的方向，而负梯度方向则表示函数下降最快的方向。因此，我们通过不断地沿着负梯度方向更新参数，使得误差平方和函数的值逐渐减小。

### 公式及步骤

通常我们将目标函数定义为 $J(k,b)=\frac{1}{2n}\sum_{i = 1}^{n}(y_i - (kx_i + b))^2$，这里乘以 $\frac{1}{2}$ 是为了在求导时计算更简便。

梯度下降法的更新公式如下：

- 关于 $b$ 的更新公式：
  $$b := b-\alpha\frac{\partial J}{\partial b}$$

- 关于 $k$ 的更新公式：
  $$k := k-\alpha\frac{\partial J}{\partial k}$$

其中，$\alpha$ 是学习率（Learning Rate），它控制了每次参数更新的步长。$\frac{\partial J}{\partial b}$
和 $\frac{\partial J}{\partial k}$ 分别是目标函数 $J(k,b)$ 关于 $b$ 和 $k$ 的偏导数。

对 $J(k,b)$ 求偏导数可得：

- $\frac{\partial J}{\partial b}=\frac{1}{n}\sum_{i = 1}^{n}(y_i - (kx_i + b))$
- $\frac{\partial J}{\partial k}=\frac{1}{n}\sum_{i = 1}^{n}(y_i - (kx_i + b))x_i$

实现梯度下降法的步骤如下：

1. **初始化参数**：随机初始化 $k$ 和 $b$ 的值。
2. **计算梯度**：根据上述偏导数公式计算 $\frac{\partial J}{\partial b}$ 和 $\frac{\partial J}{\partial k}$。
3. **更新参数**：依据梯度下降的更新公式更新 $k$ 和 $b$ 的值。
4. **重复步骤 2 和 3**：持续迭代，直到满足停止条件，例如达到最大迭代次数、目标函数的变化小于某个阈值等。

### 优缺点

- **优点**：适用于大规模数据集，因为它是一种迭代算法，每次只需处理一小部分数据，从而可以有效减少内存和计算资源的消耗。此外，梯度下降法具有较强的通用性，不仅适用于线性回归问题，还可以用于处理各种复杂的可微目标函数。
- **缺点**：学习率 $\alpha$
  的选择至关重要，如果设置过大，可能会导致算法无法收敛，甚至出现发散的情况；如果设置过小，算法的收敛速度会变得非常缓慢。另外，对于非凸的目标函数，梯度下降法可能会陷入局部最小值，而无法找到全局最小值。

### 不同类型

- **批量梯度下降（Batch Gradient Descent）**：在每次迭代时，使用全部的训练数据来计算梯度并更新参数。这种方法能够保证收敛到全局最优解，但计算量较大，尤其是在数据量庞大时，计算效率较低。
- **随机梯度下降（Stochastic Gradient Descent）**：每次迭代仅使用一个样本点来计算梯度并更新参数。该方法计算速度快，但收敛过程可能会比较震荡，不一定能收敛到全局最优解。
- **小批量梯度下降（Mini - Batch Gradient Descent）**：每次迭代使用一小部分样本点（如 32、64
  等）来计算梯度并更新参数。这种方法结合了批量梯度下降和随机梯度下降的优点，既可以减少计算量，又能保证收敛的相对稳定性。 