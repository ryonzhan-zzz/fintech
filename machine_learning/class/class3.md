### 回归（2 - 19页）
1. **回归概念（2页）**：对连续值输入的线性函数类，单变量线性回归是拟合直线。
2. **单变量线性回归（3 - 14页）**
    - 模型形式为$y = w_1x + w_0$，$w_0$和$w_1$是权重 。
    - 用平方损失函数找最佳权重，其损失函数是凸函数。
3. **梯度下降（8 - 17页）**
    - 用于复杂模型无解析解时，按梯度反方向移动找最小损失。
    - 学习率可固定或衰减，有批量和随机梯度下降两种方式。
4. **多变量线性回归（20 - 27页）**
    - 样本是多维向量，假设函数为$h_{sw}(x_j)=w\cdot x_j$ 。
    - 可用梯度下降或解析方法找最佳权重向量。

### 线性分类器（28 - 47页）
1. **线性分类器（硬阈值）（28 - 38页）**
    - 根据线性函数划分两类，分类假设基于阈值。
    - 用感知机学习规则更新权重，数据线性可分时收敛，非线性可分有挑战。
2. **线性分类器（逻辑回归）（39 - 47页）**
    - 用逻辑函数改进硬阈值问题，输出可解释为概率。
    - 用梯度下降计算权重更新，应用广泛。

### 从误差率到损失（48 - 59页）
1. **损失函数定义（50 - 51页）**：表示预测与真实值的效用损失，有多种常见类型。
2. **期望损失与最佳假设（52 - 53页）**：选期望损失最小的假设，用经验损失估计。
3. **误差来源（54 - 57页）**：包括不可实现性、方差、噪声和计算复杂度。
4. **不同规模学习的误差特点（58 - 59页）**：小规模学习误差源于近似和估计，大规模学习受计算限制。

### 过拟合与正则化（60 - 61页）
1. **过拟合问题（60页）**：多变量线性回归在高维空间可能出现过拟合。
2. **正则化方法（60 - 61页）**：通过最小化总成本解决，总成本含经验损失和复杂度。

### 概率解释（62 - 74页）
1. **线性回归的概率解释（62 - 64页）**：假设目标变量和输入关系及噪声分布，确定$y_i$概率分布。
2. **逻辑回归的概率解释（70 - 74页）**：有似然函数和对数似然函数，通过梯度上升更新权重。 